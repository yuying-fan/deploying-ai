{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe851e5",
   "metadata": {},
   "source": [
    "# AI as Judge\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM as a judge to evaluate LLM outputs. The evaluation can be based on any criteria. G-Eval is implemented by a library called [DeepEval](https://deepeval.com/) which includes a broader set of tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0650f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd5ec5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../05_src/documents/the_blue_cross.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m blue_cross_file = \u001b[33m\"\u001b[39m\u001b[33mthe_blue_cross.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m file_path = os.path.join(document_folder, blue_cross_file)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      9\u001b[39m     blue_cross_text = f.read()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/CODING/DSI/deploying-ai/deploying-ai-env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:343\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    338\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    339\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    340\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    341\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../../05_src/documents/the_blue_cross.txt'"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "document_folder = \"../../05_src/documents/\"\n",
    "blue_cross_file = \"the_blue_cross.txt\"\n",
    "file_path = os.path.join(document_folder, blue_cross_file)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    blue_cross_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843da553",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that summarizes works of fiction with a quirky and bubbly approach.\"\n",
    "PROMPT = \"\"\"\n",
    "    Summarize the following story in at most four paragraphs. Please include all key characters and plot points.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    In addition to the summary, add an introduction paragraph where you greet the reader and a conclusion where you share an opinion about the story.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f951529",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "client = OpenAI(base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1', \n",
    "                api_key='any value',\n",
    "                default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')})\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=1.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269743e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae503b",
   "metadata": {},
   "source": [
    "# Answer Relevancy\n",
    "\n",
    "The answer relevancy metric evaluates how relevant the actual output of the LLM app is compared to the provided input. This metric is self-explaining in the sense that the output includes a reason for the metric score.\n",
    "\n",
    "The metric is calculated as:\n",
    "\n",
    "$$\n",
    "AnswerRelevancy=\\frac{NumberRelevantStatements}{TotalStatements}\n",
    "$$\n",
    "\n",
    "Reference: [Answer Relevancy](https://deepeval.com/docs/metrics-answer-relevancy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cb6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval.models import GPTModel\n",
    "\n",
    "model = GPTModel(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0,\n",
    "    # api_key='any value',\n",
    "    default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')},\n",
    "    base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1',\n",
    ")\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    include_reason=True,\n",
    "    model=model,\n",
    "    \n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7de7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.measure(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73c9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(f'**Score**: {metric.score}'))\n",
    "display(Markdown(f'**Reason**: {metric.reason}'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa92303",
   "metadata": {},
   "source": [
    "# Other Metrics\n",
    "\n",
    "Other useful metric functions include:\n",
    "\n",
    "+ [Faithfulness](https://deepeval.com/docs/metrics-faithfulness): evaluates whether the `actual_output` factually aligns with the contents of  `retrieval_context`. \n",
    "+ [Contextual Precision](https://deepeval.com/docs/metrics-contextual-precision): evaluates whether nodes in your `retrieval_context` that are relevant to the given input are ranked higher than irrelevant ones. \n",
    "+ [Contextual Recall](https://deepeval.com/docs/metrics-contextual-recall): evaluates the extent of which the retrieval_context aligns with the expected_output. \n",
    "+ [Contextual Relevancy](https://deepeval.com/docs/metrics-contextual-relevancy): evaluates the overall relevance of the information presented in your retrieval_context for a given input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7b6ba",
   "metadata": {},
   "source": [
    "# G-Eval\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM-as-a-judge with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria. The G-Eval metric is the most versatile type of metric deepeval offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34e63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that specializes in works of fiction.\"\n",
    "PROMPT = \"\"\"\n",
    "    Based on the story below, answer the question provided.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    <question>\n",
    "    Who is the main antagonist in the story and what motivates their actions?\n",
    "    </question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df04e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7833729e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0eb8ad",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "The most straightforward way to establish a metric is by using a single criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a310a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the context.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0312796",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9542714",
   "metadata": {},
   "source": [
    "## Evaluation Steps \n",
    "\n",
    "G-Eval is flexible in many ways: notice that we can establish an evaluation criteria or a set of evaluation steps, that can help in guiding the model to follow specific steps to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8647c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'input'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are not OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    "    model=model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e44fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "result = evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa52080",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.model_dump()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
